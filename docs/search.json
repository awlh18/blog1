[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "From Linear to Generalized Linear Models",
    "section": "",
    "text": "As data scientists, we are often interested in finding relationships between different variables in our datasets. For instance, we might want to find out how one’s education level can impact future earnings or how movement in the US stock market is related to economic indicators such as the country’s gross domestic product (‘GDP’).\nOne of the commonly used tool to answer such questions is the linear regression model. At its core, a linear regression model seeks to establish a linear relationship between a dependent variable Y and one or more explanatory variables X.\nThe mathematical formula of a linear regression is expressed as:\n\\(Yi = \\beta_0 + \\beta_1X_{1,i} + \\beta_2X_{2,i} + \\dots + \\beta_kX_{k,i} + \\varepsilon_{i}\\)\nWhere:\n\n\\(Yi\\): the dependent variable (the outcome we are interested in - annual salary or the S&P 500 index from the examples above)\n\n\\(\\beta_0\\): the intercept of the line (the ‘starting’ or ‘base’ value of Y - what Y is when the X’s all are zero)\n\n\\(\\beta_1, \\beta_2, \\dots, \\beta_k\\): the coefficients of the explanatory variables (how much each X contribute to Y)\n\n\\(\\varepsilon_{i}\\): the error term (the variation in Y that is not explained by the X’s in the model)\n\nTo illustrate with an example, say we would like to understand how education level and years of experience in the current job impacts a person’s annual salary. To answer this question, we can set up a linear regression model:\n\\(\\text{Annual Salary}_i = \\beta_0 + \\beta_1\\text{Years of Education}_i + \\beta_2\\text{Years in Current Job}_{i} + \\varepsilon_{i}\\)\nWhere:\n\n\\(Yi\\): the annual salary for the ith person\n\n\\(\\beta_0\\): the predicted salary of a person when 0 years of education and 0 years of experience in the current job (do note the intercept is mathematically necessary for the equation, but it doesn’t always make sense in real-world contexts)\n\n\\(\\beta_1\\): the increase in salary for each additional year of education\n\n\\(\\beta_2\\): the increase in salary for each additional year of experience\n\n\\(\\varepsilon_{i}\\): the error term (the difference between the actual salary and the predicted salary)\n\n\n\nWhile linear regression is versatile and simple to interpret, it relies on several key assumptions that may limit its applicability to real-world scenarios.\n\nLinearity: the relationship between the response variable (Y) and explanatory variables (X) are assumed to be linear. In other words, the response variable (Y) is expressed as a sum of the products of each individual explanatory variable and their corresponding coefficient term. This means changes in X’s have a consistent effect on Y.\nNormality: the error terms (the difference between the predicted values and observed values) are assumed to be normally distributed.\nIndependence of errors: the error terms are independent of each other, meaning error made in predicting one observation should not be related to the error made in predicting another observation.\nHomoscedasticity: the error terms have all the same variance: \\(Var(\\varepsilon_{i}) = \\sigma^2\\)\n\nOne of the key restrictions of linear regression, particularly as a result of the linearity assumption, is that the model allows the response variable to take on any real value.\nHowever, this is often not the case in many scenarios where the response variable is binary or discrete by nature. For example, we may want to predict whether a customer will churn based on their past shopping behaviour (the response variable is binary - yes or no) or we want to model the number of car accidents in the city based on factors like weather or traffic volumne (the response variable here will be count data which are non-negative integers)."
  },
  {
    "objectID": "posts/welcome/index.html#introduction",
    "href": "posts/welcome/index.html#introduction",
    "title": "From Linear to Generalized Linear Models",
    "section": "",
    "text": "As data scientists, we are often interested in finding relationships between different variables in our datasets. For instance, we might want to find out how one’s education level can impact future earnings or how movement in the US stock market is related to economic indicators such as the country’s gross domestic product (‘GDP’).\nOne of the commonly used tool to answer such questions is the linear regression model. At its core, a linear regression model seeks to establish a linear relationship between a dependent variable Y and one or more explanatory variables X.\nThe mathematical formula of a linear regression is expressed as:\n\\(Yi = \\beta_0 + \\beta_1X_{1,i} + \\beta_2X_{2,i} + \\dots + \\beta_kX_{k,i} + \\varepsilon_{i}\\)\nWhere:\n\n\\(Yi\\): the dependent variable (the outcome we are interested in - annual salary or the S&P 500 index from the examples above)\n\n\\(\\beta_0\\): the intercept of the line (the ‘starting’ or ‘base’ value of Y - what Y is when the X’s all are zero)\n\n\\(\\beta_1, \\beta_2, \\dots, \\beta_k\\): the coefficients of the explanatory variables (how much each X contribute to Y)\n\n\\(\\varepsilon_{i}\\): the error term (the variation in Y that is not explained by the X’s in the model)\n\nTo illustrate with an example, say we would like to understand how education level and years of experience in the current job impacts a person’s annual salary. To answer this question, we can set up a linear regression model:\n\\(\\text{Annual Salary}_i = \\beta_0 + \\beta_1\\text{Years of Education}_i + \\beta_2\\text{Years in Current Job}_{i} + \\varepsilon_{i}\\)\nWhere:\n\n\\(Yi\\): the annual salary for the ith person\n\n\\(\\beta_0\\): the predicted salary of a person when 0 years of education and 0 years of experience in the current job (do note the intercept is mathematically necessary for the equation, but it doesn’t always make sense in real-world contexts)\n\n\\(\\beta_1\\): the increase in salary for each additional year of education\n\n\\(\\beta_2\\): the increase in salary for each additional year of experience\n\n\\(\\varepsilon_{i}\\): the error term (the difference between the actual salary and the predicted salary)\n\n\n\nWhile linear regression is versatile and simple to interpret, it relies on several key assumptions that may limit its applicability to real-world scenarios.\n\nLinearity: the relationship between the response variable (Y) and explanatory variables (X) are assumed to be linear. In other words, the response variable (Y) is expressed as a sum of the products of each individual explanatory variable and their corresponding coefficient term. This means changes in X’s have a consistent effect on Y.\nNormality: the error terms (the difference between the predicted values and observed values) are assumed to be normally distributed.\nIndependence of errors: the error terms are independent of each other, meaning error made in predicting one observation should not be related to the error made in predicting another observation.\nHomoscedasticity: the error terms have all the same variance: \\(Var(\\varepsilon_{i}) = \\sigma^2\\)\n\nOne of the key restrictions of linear regression, particularly as a result of the linearity assumption, is that the model allows the response variable to take on any real value.\nHowever, this is often not the case in many scenarios where the response variable is binary or discrete by nature. For example, we may want to predict whether a customer will churn based on their past shopping behaviour (the response variable is binary - yes or no) or we want to model the number of car accidents in the city based on factors like weather or traffic volumne (the response variable here will be count data which are non-negative integers)."
  },
  {
    "objectID": "posts/welcome/index.html#introduction-to-generalized-linear-models",
    "href": "posts/welcome/index.html#introduction-to-generalized-linear-models",
    "title": "From Linear to Generalized Linear Models",
    "section": "Introduction to Generalized Linear Models",
    "text": "Introduction to Generalized Linear Models\nThe limitations of linear regression lead us to consider more flexible models and this is where Generalized Linear Models (‘GLMs’) come into play. GLMs is a broad class of models that extend traditional linear regression to handle a variety of response variables beyond just continuous data.\nA GLM consists of three components:\n\nRandom component: Each response \\(Yi, \\dots, Yn\\) is a random variable with its respective mean \\(\\mu_i\\)\nSystematic component: How the k explanatory variables come into the model denoted as a linear combination: \\(\\eta_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\ldots + \\beta_k X_{i,k}\\)\nLink function: The element that connects the random component with the systematic component \\(\\eta_i\\). The connection is made through a function of \\(\\eta_i: h(\\eta_i) = \\mu_i\\)\n\nLet us look at two common GLMs, the Logistic Regression and the Poisson Regression.\n\nLogistic Regression\nThe logistic regression is widely used in situations where the response variable is binary or categorical. Unlike linear regression, which predicts continuous outcomes, logistic regression predicts probabilities. Through the logit link function, it models the probability of an event occurring (e.g., success or failure, yes or no) into a linear relationship with the explanatory variables.\n\nRandom component: Each response variable \\(Yi\\) follows a Bernoulli distribution with its respective probability of success \\(pi\\)\nSystematic component: The k explanatory variables come into the model denoted as a linear combination: \\(\\eta_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\ldots + \\beta_k X_{i,k}\\)\nLink function: The logit link function connects the systematic component to the probability of success - \\(\\eta_i = \\text{logit}(p_i) = \\ln\\left(\\frac{p_i}{1-p_i}\\right)\\)\n\nFor example:\n\nMedical Diagnostics: Predicting the probability of a patient having heart disease (yes = 1, no = 0) based on age, cholesterol level, and blood pressure.\n\nCustomer Retention: Estimating whether a customer will churn based on their activity patterns and demographic data.\n\n\nThe model outputs probabilities between 0 and 1, which can be converted into binary predictions using a decision threshold.\n\n\nPoisson Regression\nThe poisson regression is another common GLM used for modelling count data, where the response variable represents the number of events occurring within a certain timeframe (e.g., number of sales per day, accidents per year).\n\nRandom component: Each response variable \\(Yi\\) follows a Poisson distribution with its respective rate parameter \\(\\lambda_i\\)\nSystematic component: The k explanatory variables come into the model denoted as a linear combination: \\(\\eta_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\ldots + \\beta_k X_{i,k}\\)\nLink function: The log link function connects the systematic component to the rata parameter - \\(\\eta_i = \\ln(\\lambda_i)\\)\n\nFor example:\n\nHealthcare: Modeling the number of hospital visits per month based on a patient’s age and health status.\n\nTraffic Analysis: Predicting the number of road accidents at an intersection based on traffic volume and weather conditions.\n\n\n\nClosing note\nIn short, understanding linear and generalized linear models provides data scientists with powerful tools to model a wide range of data. While we’ve explored logistic and poisson regression, it’s worth noting that there are many other GLMs, each tailored to address specific types of data, making this family of models incredibly versatile."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "From Linear to Generalized Linear Models\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nAlex Wong\n\n\n\n\n\n\nNo matching items"
  }
]