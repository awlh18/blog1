---
title: "From Linear to Generalized Linear Models"
author: "Alex Wong"
date: "2025-01-18"
---

## Introduction
As data scientists, we are often interested in finding relationships between different variables in our datasets. For instance, we might want to find out how one's education level can impact future earnings or how movement in the US stock market is related to economic indicators such as the country's gross domestic product ('GDP'). 

 One of the commonly used tool to answer such questions is the linear regression model. At its core, a linear regression model seeks to establish a linear relationship between a dependent variable Y and one or more explanatory variables X. The model finds a line or hyperplane through the data points by minimizing the difference between the predicted values and the actual observed values. This difference is quantified using a metric called sum of squared errors ('SSE'), and the model uses a method called ordinary least squares ('OLS') to minimize it.
 
 The mathematical formula of a linear regression is expressed as:

 $Yi = \beta_0 + \beta_1X_{1,i} + \beta_2X_{2,i} + \dots + \beta_kX_{k,i} + \varepsilon_{i}$

 Where:

- **$Yi$:**  the dependent variable (the outcome we are interested in - annual salary or the S&P 500 index from the examples above) \
- **$\beta_0$:**  the intercept of the line (the 'starting' or 'base' value of Y - what Y is when all X's are zero) \
- **$\beta_1, \beta_2, \dots, \beta_k$:**  the coefficients of the explanatory variables (how much each X contribute to Y) \
- **$\varepsilon_{i}$:**  the error term (the variation in Y that is not explained by the X's in the model)

To illustrate with an example, say we would like to understand how education level and years of experience in the current job impacts a person's annual salary when they are 35. To answer this question, we can set up a linear regression model: 

$\text{Annual Salary}_i = \beta_0 + \beta_1\text{Years of Education}_i + \beta_2\text{Years in Current Job}_{i} + \varepsilon_{i}$

 Where:

- **$Yi$:**  the annual salary for the ith person \
- **$\beta_0$:**  the predicted salary of a person when 0 years of education and 0 years of experience in the current job (do note the intercept is mathematically necessary for the equation, but it doesn’t always make sense in real-world contexts) \
- **$\beta_1$:**  the increase in salary for each additional year of education  \
- **$\beta_2$:**  the increase in salary for each additional year of experience  \
- **$\varepsilon_{i}$:**  the error term (the difference between the actual salary and the predicted salary) 


#### Linear regression assumptions
While linear regression is versatile and simple to interpret, it relies on several key assumptions that may limit its applicability in real-world scenarios. 

1. **Linearity:** the relationship between the response variable (Y) and explanatory variables (X) are assumed to be linear. In other words, the response variable (Y) is expressed as a sum of the products of each individual explanatory variable and their corresponding coefficient term. This means changes in X's have a consistent effect on Y. 

2. **Normality:** the error terms (the difference between the predicted values and observed values) are assumed to be normally distributed. 

3. **Independence of errors:** - the error terms are independent of each other, meaning error made in predicting one observation should not be related to the error made in predicting another observation.

4. **Homoscedasticity:** the error terms have all the same variance: $Var(\varepsilon_{i}) = \sigma^2$

One of the key restrictions of linear regression, particularly as a result of the linearity assumption, is that the model allows the response variable to take on any real value. However, this is often not the case in many scenarios where the response variable in question may have more restrictive nature. For example, we may want to predict whether a customer will churn based on their past interactions with the company (the response variable is binary - yes or no) or we want to model the number of car accidents in the city based on factors like weather, traffic volumne etc. (the response variable here will be count data which are non-negative integers). 

## Introduction to Generalized Linear Models 

The limitations of linear regression lead us to consider more flexible models and this is where Generalized Linear Models ('GLMs') come into play. GLMs a broad class of models that extend traditional linear regression to handle a variety of response variables beyond just continuous data. 

A GLM consists of three components: 

1. **Random component**: Each response $Yi, \dots, Yn$ is a random variable with its respective mean $\mu_i$
2. **Systematic component**: How the k explanatory variables come into the model denoted as a linear combination: $\eta_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \ldots + \beta_k X_{i,k}$
3. **Link function**: The element that connects the random component with the systematic component $\eta_i$. The connection is made through a function of $\eta_i: h(\eta_i) = \mu_i$

Let us look at two common GLMs, the Logistic Regression, and the Poisson Regression. 


### Logistic Regression 

The logistic regression is widely used in situations where the response variable is binary or categorical. Unlike linear regression, which predicts continuous outcomes, logistic regression predicts probabilities. Through the logit link function, it models the probability of an event occurring (e.g., success or failure, yes or no) into a linear relationship with the explanatory variables.

1. **Random component**: Each response variable $Yi$ follows a Bernoulli distribution with its respective probability of success $pi$
2. **Systematic component**: The k explanatory variables come into the model denoted as a linear combination: $\eta_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \ldots + \beta_k X_{i,k}$
3. **Link function**: The logit link function connects the systematic component to the probability of success - $\eta_i = \text{logit}(p_i) = \ln\left(\frac{p_i}{1-p_i}\right)$

For example:

- **Medical Diagnostics:** Predicting the probability of a patient having heart disease (yes = 1, no = 0) based on age, cholesterol level, and blood pressure. \
- **Customer Retention:** Estimating whether a customer will churn based on their activity patterns and demographic data.\

The model outputs probabilities between 0 and 1, which can be converted into binary predictions using a decision threshold. 


### Poisson Regression 

The poisson regression is another common GLM used for modelling count data, where the response variable represents the number of events occurring within a certain timeframe (e.g., number of sales per day, accidents per year). 

1. **Random component**: Each response variable $Yi$ follows a Poisson distribution with its respective rate parameter $\lambda_i$
2. **Systematic component**: The k explanatory variables come into the model denoted as a linear combination: $\eta_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \ldots + \beta_k X_{i,k}$
3. **Link function**: The log link function connects the systematic component to the rata parameter - $\eta_i = \ln(\lambda_i)$


For example:

- **Healthcare:** Modeling the number of hospital visits per month based on a patient’s age and health status. \
- **Traffic Analysis:** Predicting the number of road accidents at an intersection based on traffic volume and weather conditions.

### Closing note
In short, understanding linear and generalized linear models provides data scientists with powerful tools to model a wide range of data. While we’ve explored logistic and poisson regression, it’s worth noting that there are many other GLMs, each tailored to address specific types of data and research questions, making this family of models incredibly versatile.